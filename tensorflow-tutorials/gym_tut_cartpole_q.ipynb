{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fbb9e97d-da1e-4ea7-9c37-f657d50dc301",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gym version: 0.29.1\n",
      "Tensorflow version: 2.16.1\n"
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from datetime import datetime\n",
    "from sklearn.pipeline import FeatureUnion\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.kernel_approximation import RBFSampler\n",
    "\n",
    "print(f\"Gym version: {gym.__version__}\")\n",
    "print(f\"Tensorflow version: {tf.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c35b2ff1-02fb-4e9a-b19d-bf45a872e90f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#class SGDRegressor:\n",
    "#  def __init__(self, D):\n",
    "#    self.w = np.random.randn(D) / np.sqrt(D)\n",
    "#    self.lr = 0.1\n",
    "#\n",
    "#  def partial_fit(self, X, Y):\n",
    "#    self.w += self.lr*(Y - X.dot(self.w)).dot(X)\n",
    "#\n",
    "#  def predict(self, X):\n",
    "#    return X.dot(self.w)\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "dd6c6b86-c4c1-417e-870a-bd003401acb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SGDRegressor:\n",
    "    def __init__(self, D):\n",
    "        print(\"Hello TensorFlow!\")\n",
    "        lr = 0.1\n",
    "    \n",
    "        inputs = tf.keras.Input(shape=(D,), dtype=tf.float32, name='X')\n",
    "        outputs = tf.keras.layers.Dense(1, activation=\"linear\")(inputs)\n",
    "        self.model = tf.keras.Model(inputs=inputs, outputs=outputs)\n",
    "        self.optimizer = tf.keras.optimizers.SGD(learning_rate=lr)\n",
    "\n",
    "    def partial_fit(self, X, Y):\n",
    "        with tf.GradientTape() as tape:\n",
    "            y = self.model(X)\n",
    "        loss = tf.reduce_mean(tf.square(tf.subtract(y, Y)))\n",
    "        grads = tape.gradient(loss, self.model.trainable_variables)\n",
    "        self.optimizer.apply(grads, self.model.trainable_variables)\n",
    "\n",
    "    def predict(self, X):\n",
    "        return self.model(X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "38e2fe66-b862-47bf-a274-a679778aa8b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeatureTransformer:\n",
    "  def __init__(self, env):\n",
    "    # observation_examples = np.array([env.observation_space.sample() for x in range(10000)])\n",
    "    # NOTE!! state samples are poor, b/c you get velocities --> infinity\n",
    "    observation_examples = np.random.random((20000, 4))*2 - 1\n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(observation_examples)\n",
    "\n",
    "    # Used to converte a state to a featurizes represenation.\n",
    "    # We use RBF kernels with different variances to cover different parts of the space\n",
    "    featurizer = FeatureUnion([\n",
    "            (\"rbf1\", RBFSampler(gamma=0.05, n_components=1000)),\n",
    "            (\"rbf2\", RBFSampler(gamma=1.0, n_components=1000)),\n",
    "            (\"rbf3\", RBFSampler(gamma=0.5, n_components=1000)),\n",
    "            (\"rbf4\", RBFSampler(gamma=0.1, n_components=1000))\n",
    "            ])\n",
    "    feature_examples = featurizer.fit_transform(scaler.transform(observation_examples))\n",
    "\n",
    "    self.dimensions = feature_examples.shape[1]\n",
    "    self.scaler = scaler\n",
    "    self.featurizer = featurizer\n",
    "\n",
    "  def transform(self, observations):\n",
    "    scaled = self.scaler.transform(observations)\n",
    "    return self.featurizer.transform(scaled)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "e6d13497-3f76-4898-b6fd-06bf0329fd77",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model:\n",
    "  def __init__(self, env, feature_transformer):\n",
    "    self.env = env\n",
    "    self.models = []\n",
    "    self.feature_transformer = feature_transformer\n",
    "    for i in range(env.action_space.n):\n",
    "      model = SGDRegressor(feature_transformer.dimensions)\n",
    "      self.models.append(model)\n",
    "\n",
    "  def predict(self, s):\n",
    "    X = self.feature_transformer.transform(np.atleast_2d(s))\n",
    "    result = np.stack([m.predict(X) for m in self.models]).T\n",
    "    return result\n",
    "\n",
    "  def update(self, s, a, G):\n",
    "    X = self.feature_transformer.transform(np.atleast_2d(s))\n",
    "    self.models[a].partial_fit(X, [G])\n",
    "\n",
    "  def sample_action(self, s, eps):\n",
    "    if np.random.random() < eps:\n",
    "      return self.env.action_space.sample()\n",
    "    else:\n",
    "      return np.argmax(self.predict(s))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "a603fe0e-7e85-4b96-8601-d2347d874908",
   "metadata": {},
   "outputs": [],
   "source": [
    "def play_one(env, model, eps, gamma):\n",
    "  observation, _ = env.reset()\n",
    "  done = False\n",
    "  totalreward = 0\n",
    "  iters = 0\n",
    "  while not done and iters < 2000:\n",
    "    # if we reach 2000, just quit, don't want this going forever\n",
    "    # the 200 limit seems a bit early\n",
    "    action = model.sample_action(observation, eps)\n",
    "    prev_observation = observation\n",
    "    observation, reward, done, info, _ = env.step(action)\n",
    "\n",
    "    if done:\n",
    "      reward = -200\n",
    "\n",
    "    # update the model\n",
    "    next = model.predict(observation)\n",
    "    # print(next.shape)\n",
    "    #assert(next.shape == (1, env.action_space.n))\n",
    "    G = reward + gamma*np.max(next)\n",
    "    model.update(prev_observation, action, G)\n",
    "\n",
    "    if reward == 1: # if we changed the reward to -200\n",
    "      totalreward += reward\n",
    "    iters += 1\n",
    "\n",
    "  return totalreward\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "d41fd5c1-184a-4066-9d9a-cb5694a4508d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_running_avg(totalrewards):\n",
    "  N = len(totalrewards)\n",
    "  running_avg = np.empty(N)\n",
    "  for t in range(N):\n",
    "    running_avg[t] = totalrewards[max(0, t-100):(t+1)].mean()\n",
    "  plt.plot(running_avg)\n",
    "  plt.title(\"Running Average\")\n",
    "  plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "47659c3a-6064-4ce4-8fad-3a042e9e9209",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello TensorFlow!\n",
      "Hello TensorFlow!\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "No gradients provided for any variable.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[47], line 11\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m n \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(N):\n\u001b[1;32m     10\u001b[0m     eps \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1.0\u001b[39m\u001b[38;5;241m/\u001b[39mnp\u001b[38;5;241m.\u001b[39msqrt(n\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m---> 11\u001b[0m     totalreward \u001b[38;5;241m=\u001b[39m \u001b[43mplay_one\u001b[49m\u001b[43m(\u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgamma\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m     totalrewards[n] \u001b[38;5;241m=\u001b[39m totalreward\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m n \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m100\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "Cell \u001b[0;32mIn[43], line 21\u001b[0m, in \u001b[0;36mplay_one\u001b[0;34m(env, model, eps, gamma)\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# print(next.shape)\u001b[39;00m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m#assert(next.shape == (1, env.action_space.n))\u001b[39;00m\n\u001b[1;32m     20\u001b[0m G \u001b[38;5;241m=\u001b[39m reward \u001b[38;5;241m+\u001b[39m gamma\u001b[38;5;241m*\u001b[39mnp\u001b[38;5;241m.\u001b[39mmax(\u001b[38;5;28mnext\u001b[39m)\n\u001b[0;32m---> 21\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprev_observation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maction\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mG\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m reward \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m: \u001b[38;5;66;03m# if we changed the reward to -200\u001b[39;00m\n\u001b[1;32m     24\u001b[0m   totalreward \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m reward\n",
      "Cell \u001b[0;32mIn[42], line 17\u001b[0m, in \u001b[0;36mModel.update\u001b[0;34m(self, s, a, G)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mupdate\u001b[39m(\u001b[38;5;28mself\u001b[39m, s, a, G):\n\u001b[1;32m     16\u001b[0m   X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeature_transformer\u001b[38;5;241m.\u001b[39mtransform(np\u001b[38;5;241m.\u001b[39matleast_2d(s))\n\u001b[0;32m---> 17\u001b[0m   \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodels\u001b[49m\u001b[43m[\u001b[49m\u001b[43ma\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpartial_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43mG\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[46], line 16\u001b[0m, in \u001b[0;36mSGDRegressor.partial_fit\u001b[0;34m(self, X, Y)\u001b[0m\n\u001b[1;32m     14\u001b[0m loss \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mreduce_mean(tf\u001b[38;5;241m.\u001b[39msquare(tf\u001b[38;5;241m.\u001b[39msubtract(y, Y)))\n\u001b[1;32m     15\u001b[0m grads \u001b[38;5;241m=\u001b[39m tape\u001b[38;5;241m.\u001b[39mgradient(loss, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mtrainable_variables)\n\u001b[0;32m---> 16\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrainable_variables\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/jupyterlab-debugger/lib/python3.12/site-packages/keras/src/optimizers/base_optimizer.py:335\u001b[0m, in \u001b[0;36mBaseOptimizer.apply\u001b[0;34m(self, grads, trainable_variables)\u001b[0m\n\u001b[1;32m    328\u001b[0m grads, trainable_variables \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    329\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_overwrite_variables_directly_with_gradients(\n\u001b[1;32m    330\u001b[0m         grads, trainable_variables\n\u001b[1;32m    331\u001b[0m     )\n\u001b[1;32m    332\u001b[0m )\n\u001b[1;32m    334\u001b[0m \u001b[38;5;66;03m# Filter empty gradients.\u001b[39;00m\n\u001b[0;32m--> 335\u001b[0m grads, trainable_variables \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_filter_empty_gradients\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    336\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrainable_variables\u001b[49m\n\u001b[1;32m    337\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    338\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mlist\u001b[39m(grads)) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    339\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/jupyterlab-debugger/lib/python3.12/site-packages/keras/src/optimizers/base_optimizer.py:662\u001b[0m, in \u001b[0;36mBaseOptimizer._filter_empty_gradients\u001b[0;34m(self, grads, vars)\u001b[0m\n\u001b[1;32m    659\u001b[0m         missing_grad_vars\u001b[38;5;241m.\u001b[39mappend(v\u001b[38;5;241m.\u001b[39mname)\n\u001b[1;32m    661\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m filtered_grads:\n\u001b[0;32m--> 662\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo gradients provided for any variable.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    663\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m missing_grad_vars:\n\u001b[1;32m    664\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    665\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGradients do not exist for variables \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    666\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mreversed\u001b[39m(missing_grad_vars))\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m when minimizing the loss.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    667\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m If using `model.compile()`, did you forget to provide a \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    668\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`loss` argument?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    669\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: No gradients provided for any variable."
     ]
    }
   ],
   "source": [
    "env = gym.make('CartPole-v1')\n",
    "ft = FeatureTransformer(env)\n",
    "model = Model(env, ft)\n",
    "gamma = 0.99\n",
    "\n",
    "N = 500\n",
    "totalrewards = np.empty(N)\n",
    "costs = np.empty(N)\n",
    "for n in range(N):\n",
    "    eps = 1.0/np.sqrt(n+1)\n",
    "    totalreward = play_one(env, model, eps, gamma)\n",
    "    totalrewards[n] = totalreward\n",
    "    if n % 100 == 0:\n",
    "        print(\"episode:\", n, \"total reward:\", totalreward, \"eps:\", eps, \"avg reward (last 100):\", totalrewards[max(0, n-100):(n+1)].mean())\n",
    "\n",
    "print(\"avg reward for last 100 episodes:\", totalrewards[-100:].mean())\n",
    "print(\"total steps:\", totalrewards.sum())\n",
    "\n",
    "plt.plot(totalrewards)\n",
    "plt.title(\"Rewards\")\n",
    "plt.show()\n",
    "\n",
    "plot_running_avg(totalrewards)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a57f079e-ef29-44cf-b9cd-8a6c8b5a81b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
